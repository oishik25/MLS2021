{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>ML Assignment 1</h3></center><br>\n",
    "<center><h5>Oishik Dasgupta</h5></center><br>\n",
    "<center><h6>February 28, 2021</h6></center>\n",
    "\n",
    "\n",
    "<b><u>Theorem:</u></b> <b>Under Gaussian Noise assumption linear regression amounts to least square</b>\n",
    "\n",
    "<u>Proof:</u>\n",
    "\\Let us assume that the target variables and the inputs are related via the equation\n",
    "$$ y_i=\\theta^Tx_i +\\epsilon_i $$\n",
    "where $\\epsilon_i$ is an error term that captures either unmodelled effects or random noise. Let us further assume that the $\\epsilon_i$ are distributed IID according to a Gaussian distribution with mean zero and some variance $\\sigma^2$. We can write this assumption as $$\\epsilon_i \\sim N(0,\\sigma^2)$$\n",
    "i.e., the density of $\\epsilon_i $ is given by $$p(\\epsilon_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left({-\\frac{\\left(\\epsilon_i\\right)^2}{2\\sigma^2}}\\right)$$ This implies that\n",
    "$$p\\left(y_i|x_i;\\theta\\right)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left({-\\frac{\\left(y_i-\\theta^Tx_i\\right)^2}{2\\sigma^2}}\\right) $$ The notation \"$p\\left(y_i|x_i;\\theta\\right)$\" indicates that this is the distribution of $y_i$ given $x_i$ and parameterized by $\\theta$. Note that we should not condition on $\\theta\\left(p\\left(y_i|x_i;\\theta\\right)\\right)$, since $\\theta$ is not a random variable.We can also write the distribution of $y_i$ as $y_i|x_i;\\theta\\sim N\\left(\\theta^Tx_i,\\sigma^2\\right)$.\n",
    "\\Given X(the design matrix, which contains all the $x_i$'s) and $\\theta$, what is the distribution of the $y_i$'s? probability of the data is given by p($\\vec{y}{\\,}|X;\\theta$). This quantity is typically viewed a function of $\\vec{y}{\\,}$ (and perhaps X), for a fixed value of $\\theta$. When we wish to explicitly view this as a function of $\\theta$, we will instead call it the likelihood function:\n",
    "$$L\\left(\\theta\\right) = L(\\theta;X,\\vec{y}{\\,}) = p\\left(\\vec{y}{\\,}|X;\\theta\\right)$$\n",
    "Note that by the independence assumption on the $\\epsilon_i$'s (and hence also the $y_i$'s given the $x_i$'s), this can also be written\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "L\\left(\\theta\\right) &=  \\prod\\limits_{i = 1}^{m} p\\left(y_i|x_i;\\theta\\right) \\\\ &=\\prod\\limits_{i = 1}^{m} \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left({-\\frac{\\left(y_i-\\theta^Tx_i\\right)^2}{2\\sigma^2}}\\right)\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "Now given this probabilistic model relating the $y_i$'s and the $x_i$'s, what is a reasonable way of choosing our best guess of the parameters $\\theta$? The principle of maximum likelihood says that we should choose $\\theta$ so as to make the data as high probability as possible. i.e.,we should choose $\\theta$ to maximize L($\\theta$).\n",
    "Instead of maximizing L($\\theta$),we can maximize any strictly increasing function of L($\\theta$). In particular, the derivations will be a bit simpler if we instead maximize the log likelihood l($\\theta$):\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "l(\\theta) &= \\log L\\left(\\theta\\right) \\\\\n",
    "&= \\log \\prod\\limits_{i = 1}^{m} \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left({-\\frac{\\left(y_i-\\theta^Tx_i\\right)^2}{2\\sigma^2}}\\right) \\\\\n",
    "&= \\sum\\limits_{i = 1}^{m} \\log \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left({-\\frac{\\left(y_i-\\theta^Tx_i\\right)^2}{2\\sigma^2}}\\right)\\\\\n",
    "&= m \\log \\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{1}{\\sigma^2}.\\frac{1}{2}\\sum\\limits_{i = 1}^{m}\\left(y_i-\\theta^Tx_i\\right)^2\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "Hence, maximizing l($\\theta$) gives the same answer as minimizing \n",
    "$$\\frac{1}{2}\\sum\\limits_{i = 1}^{m}\\left(y_i-\\theta^Tx_i\\right)^2,$$\n",
    "which we recognize to be J($\\theta$),our original least-squares cost function.\n",
    "To summarize: Under the previous probabilistic assumptions on the data, least-squares regression corresponds to finding the maximum likelihood estimate of $\\theta$.This is thus one set of assumptions under which least-squares regression can be justified as a very natural method thatâ€™s just doing MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
